{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VGG-Implementation**\n",
    "\n",
    "Small introduction about the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score,recall_score\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    \"train_images\": \"https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\": \"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\": \"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Official URL dataset extract fails - http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract files\n",
    "def download_and_extract(url, path, is_label=False):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {url}...\")\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        print(f\"Downloaded {path}\")\n",
    "    else:\n",
    "        print(f\"{path} already exists, skipping download.\")\n",
    "\n",
    "    offset = 8 if is_label else 16 # Handle labels separately as their header is 8 bytes instead of 16\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        return np.frombuffer(f.read(), np.uint8, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloaded ./data/train-images.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloaded ./data/train-labels.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloaded ./data/test-images.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz...\n",
      "Downloaded ./data/test-labels.gz\n",
      "MNIST data downloaded and loaded.\n"
     ]
    }
   ],
   "source": [
    "def load_mnist_data():\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "    train_images = download_and_extract(urls['train_images'], './data/train-images.gz')\n",
    "    train_labels = download_and_extract(urls['train_labels'], './data/train-labels.gz', is_label=True)\n",
    "    test_images = download_and_extract(urls['test_images'], './data/test-images.gz')\n",
    "    test_labels = download_and_extract(urls['test_labels'], './data/test-labels.gz', is_label=True)\n",
    "\n",
    "    # Reshape and normalize the images\n",
    "    train_images = train_images.reshape(-1, 28, 28) / 255.0\n",
    "    test_images = test_images.reshape(-1, 28, 28) / 255.0\n",
    "\n",
    "    # Combine train and test datasets for custom split\n",
    "    images = np.concatenate((train_images, test_images), axis=0)\n",
    "    labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_mnist_data()\n",
    "print(\"MNIST data downloaded and loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 42000\n",
      "Validation set size: 14000\n",
      "Test set size: 14000\n"
     ]
    }
   ],
   "source": [
    "def split_data(images, labels):\n",
    "    total_size = len(images)\n",
    "\n",
    "    # Sizes for each split\n",
    "    train_size = int(0.6 * total_size)\n",
    "    val_size = int(0.2 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_images, train_labels = images[:train_size], labels[:train_size]\n",
    "    val_images, val_labels = images[train_size:train_size+val_size], labels[train_size:train_size+val_size]\n",
    "    test_images, test_labels = images[train_size+val_size:], labels[train_size+val_size:]\n",
    "\n",
    "    return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels)\n",
    "\n",
    "# Split into training, validation, and testing sets (60%, 20%, 20%)\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = split_data(images, labels)\n",
    "\n",
    "print(f\"Training set size: {len(train_images)}\")\n",
    "print(f\"Validation set size: {len(val_images)}\")\n",
    "print(f\"Test set size: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Numpy arrays into tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        # Convert numpy arrays to torch tensors\n",
    "        self.images = torch.tensor(images, dtype=torch.float32).unsqueeze(1)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long).clone().detach()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "    def get_images(self):\n",
    "        return self.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: torch.Size([64, 1, 28, 28]), Batch label shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects\n",
    "train_dataset = MNISTDataset(train_images, train_labels)\n",
    "val_dataset = MNISTDataset(val_images, val_labels)\n",
    "test_dataset = MNISTDataset(test_images, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "images, labels = next(train_iter)\n",
    "print(f\"Batch image shape: {images.shape}, Batch label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_dataset: 42000\n",
      "Number of rows in test_dataset: 14000\n",
      "Number of rows in val_dataset: 14000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of rows in test_dataset: {len(test_dataset)}\")\n",
    "print(f\"Number of rows in val_dataset: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Model Building**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
